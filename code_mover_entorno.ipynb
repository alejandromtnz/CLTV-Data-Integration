{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyodbc in c:\\users\\aleja\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 17:06:42,520 - INFO - Conectando a Azure SQL...\n",
      "2025-03-13 17:06:42,549 - INFO - Conectando a SQL Server Local...\n",
      "2025-03-13 17:06:42,574 - INFO - Ejecutando Dim_customer.sql en Azure SQL...\n",
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_19108\\2883473097.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql_query, conn)\n",
      "2025-03-13 17:06:45,351 - INFO - Datos extraídos para Dim_customer.sql: 44053 filas\n",
      "2025-03-13 17:06:45,547 - INFO - Tabla Dim_customer eliminada si existía.\n",
      "2025-03-13 17:06:45,547 - ERROR - Error procesando Dim_customer.sql: 'DataFrame' object has no attribute 'dtype'\n",
      "2025-03-13 17:06:45,547 - INFO - Ejecutando Dim_geo.sql en Azure SQL...\n",
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_19108\\2883473097.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql_query, conn)\n",
      "2025-03-13 17:06:45,600 - INFO - Datos extraídos para Dim_geo.sql: 12 filas\n",
      "2025-03-13 17:06:45,628 - INFO - Tabla Dim_geo eliminada si existía.\n",
      "2025-03-13 17:06:45,636 - INFO - Tabla Dim_geo creada correctamente en SQL Server Local.\n",
      "2025-03-13 17:06:45,647 - INFO - 12 filas insertadas en Dim_geo.\n",
      "2025-03-13 17:06:45,647 - INFO - Ejecutando Dim_product.sql en Azure SQL...\n",
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_19108\\2883473097.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql_query, conn)\n",
      "2025-03-13 17:06:45,696 - INFO - Datos extraídos para Dim_product.sql: 404 filas\n",
      "2025-03-13 17:06:45,704 - INFO - Tabla Dim_product eliminada si existía.\n",
      "2025-03-13 17:06:45,704 - ERROR - Error procesando Dim_product.sql: 'DataFrame' object has no attribute 'dtype'\n",
      "2025-03-13 17:06:45,704 - INFO - Ejecutando Dim_t.sql en Azure SQL...\n",
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_19108\\2883473097.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql_query, conn)\n",
      "2025-03-13 17:06:45,830 - INFO - Datos extraídos para Dim_t.sql: 3652 filas\n",
      "2025-03-13 17:06:45,858 - INFO - Tabla Dim_t eliminada si existía.\n",
      "2025-03-13 17:06:45,867 - INFO - Tabla Dim_t creada correctamente en SQL Server Local.\n",
      "2025-03-13 17:06:45,974 - INFO - 3652 filas insertadas en Dim_t.\n",
      "2025-03-13 17:06:45,974 - INFO - Ejecutando Fact.sql en Azure SQL...\n",
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_19108\\2883473097.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql_query, conn)\n",
      "2025-03-13 17:06:48,927 - INFO - Datos extraídos para Fact.sql: 58049 filas\n",
      "2025-03-13 17:06:49,218 - INFO - Tabla Fact eliminada si existía.\n",
      "2025-03-13 17:06:49,218 - ERROR - Error procesando Fact.sql: 'DataFrame' object has no attribute 'dtype'\n",
      "2025-03-13 17:06:49,285 - INFO - \n",
      "¡Proceso completado!\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Conexión a **Azure SQL**\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\"\n",
    "\n",
    "# Conexión a **SQL Server LOCAL**\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'master'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\"\n",
    "\n",
    "# Ruta a la carpeta con los archivos SQL\n",
    "SQL_FOLDER_PATH = 'src/sql/'\n",
    "\n",
    "# Nombres de los archivos SQL\n",
    "sql_files = [\n",
    "    'Dim_customer.sql',\n",
    "    'Dim_geo.sql',\n",
    "    'Dim_product.sql',\n",
    "    'Dim_t.sql',\n",
    "    'Fact.sql'\n",
    "]\n",
    "\n",
    "def execute_sql_file(conn, sql_file_path):\n",
    "    \"\"\"Ejecuta un archivo SQL y devuelve un DataFrame con los resultados.\"\"\"\n",
    "    with open(sql_file_path, 'r', encoding='utf-8') as file:\n",
    "        sql_query = file.read()\n",
    "    return pd.read_sql(sql_query, conn)\n",
    "\n",
    "def create_table(conn, table_name, df):\n",
    "    \"\"\"Crea una tabla en la base de datos local basada en el DataFrame.\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        # Eliminar la tabla si ya existe\n",
    "        drop_table_sql = f\"DROP TABLE IF EXISTS {table_name}\"\n",
    "        cursor.execute(drop_table_sql)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Tabla {table_name} eliminada si existía.\")\n",
    "\n",
    "        # Crear la tabla en SQL Server Local con tipos de datos ajustados\n",
    "        columns_sql = []\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "            if np.issubdtype(col_type, np.float32) or np.issubdtype(col_type, np.float64):\n",
    "                columns_sql.append(f'[{col}] FLOAT')\n",
    "            elif np.issubdtype(col_type, np.int32) or np.issubdtype(col_type, np.int64):\n",
    "                columns_sql.append(f'[{col}] INT')\n",
    "            else:\n",
    "                columns_sql.append(f'[{col}] NVARCHAR(255)')\n",
    "                \n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            {', '.join(columns_sql)}\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_sql)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Tabla {table_name} creada correctamente en SQL Server Local.\")\n",
    "\n",
    "def insert_data(conn, table_name, df):\n",
    "    \"\"\"Inserta los datos del DataFrame en la tabla de la base de datos local.\"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        placeholders = ', '.join(['?' for _ in df.columns])\n",
    "        insert_sql = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "        cursor.fast_executemany = True\n",
    "        cursor.executemany(insert_sql, df.values.tolist())\n",
    "        conn.commit()\n",
    "        logging.info(f\"{df.shape[0]} filas insertadas en {table_name}.\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Conectar a Azure SQL\n",
    "        logging.info(\"Conectando a Azure SQL...\")\n",
    "        conn_azure = pyodbc.connect(azure_conn_str)\n",
    "\n",
    "        # Conectar a SQL Server Local\n",
    "        logging.info(\"Conectando a SQL Server Local...\")\n",
    "        conn_local = pyodbc.connect(local_conn_str)\n",
    "\n",
    "        for sql_file in sql_files:\n",
    "            try:\n",
    "                sql_file_path = os.path.join(SQL_FOLDER_PATH, sql_file)\n",
    "                logging.info(f\"Ejecutando {sql_file} en Azure SQL...\")\n",
    "\n",
    "                # Ejecutar la consulta y obtener los datos\n",
    "                df = execute_sql_file(conn_azure, sql_file_path)\n",
    "\n",
    "                if df.empty:\n",
    "                    logging.warning(f\"La consulta no devolvió resultados para {sql_file}. No se creará la tabla en SQL Server Local.\")\n",
    "                    continue\n",
    "\n",
    "                logging.info(f\"Datos extraídos para {sql_file}: {df.shape[0]} filas\")\n",
    "\n",
    "                # Convertir NaN en columnas numéricas a 0\n",
    "                df = df.fillna(0)\n",
    "\n",
    "                # Convertir valores numéricos problemáticos\n",
    "                for col in df.select_dtypes(include=['float64']).columns:\n",
    "                    df[col] = df[col].astype(np.float32)  # Reducir precisión\n",
    "\n",
    "                for col in df.select_dtypes(include=['int64']).columns:\n",
    "                    df[col] = df[col].astype(np.int32)  # Evitar valores fuera de rango\n",
    "\n",
    "                # Crear la tabla e insertar los datos\n",
    "                table_name = sql_file.split('.')[0]\n",
    "                create_table(conn_local, table_name, df)\n",
    "                insert_data(conn_local, table_name, df)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error procesando {sql_file}: {e}\")\n",
    "                continue  # Continuar con el siguiente archivo\n",
    "\n",
    "    except pyodbc.Error as e:\n",
    "        logging.error(f\"Error de conexión a la base de datos: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inesperado: {e}\")\n",
    "    finally:\n",
    "        if 'conn_azure' in locals():\n",
    "            conn_azure.close()\n",
    "        if 'conn_local' in locals():\n",
    "            conn_local.close()\n",
    "\n",
    "    logging.info(\"\\n¡Proceso completado!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Azure SQL...\n",
      "Ejecutando consulta en Azure SQL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_19108\\2914783269.py:59: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(SQL_QUERY, conn_azure)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Datos extraídos: 44053 filas\n",
      "Conectando a SQL Server Local...\n",
      "   - Tabla eliminada si existía.\n",
      "   - Tabla FACT_SALES creada correctamente en SQL Server Local.\n",
      "   - 44053 filas insertadas en FACT_SALES.\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Conexión a **Azure SQL**\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\"\n",
    "\n",
    "# Conexión a **SQL Server LOCAL**\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'master'\n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\"\n",
    "\n",
    "# Ruta a la carpeta con los archivos SQL\n",
    "SQL_FOLDER_PATH = 'src/sql/'\n",
    " \n",
    "#  Consulta SQL en Azure SQL\n",
    "SQL_QUERY = \"\"\"\n",
    "  SELECT\n",
    "    Customer_ID,\n",
    "    Edad,\n",
    "    Fecha_nacimiento,\n",
    "    GENERO,\n",
    "    CAST(codigopostalid AS INT) AS CP,\n",
    "    poblacion,\n",
    "    provincia,\n",
    "    STATUS_SOCIAL,\n",
    "    [RENTA_MEDIA_ESTIMADA],\n",
    "    [ENCUESTA_ZONA_CLIENTE_VENTA],\n",
    "    [ENCUESTA_CLIENTE_ZONA_TALLER],\n",
    "    [A], [B], [C], [D], [E], [F], [G], [H], [I], [J], [K], [U2],\n",
    "    [Max_Mosaic_G], [Max_Mosaic2], [Renta_Media], [F2], [Mosaic_number]\n",
    "  FROM\n",
    "    [DATAEX].[003_clientes] cliente\n",
    "  LEFT JOIN  \n",
    "    [DATAEX].[005_cp] cp ON cliente.CODIGO_POSTAL = cp.CP\n",
    "  LEFT JOIN\n",
    "    [DATAEX].[019_mosaic] mosaic ON try_cast(cp.codigopostalid AS INT) = try_cast(mosaic.CP AS INT)\n",
    "\"\"\"\n",
    " \n",
    "# 🔹 Nombre de la tabla en SQL Server Local\n",
    "NEW_TABLE_NAME = \"FACT_SALES\"\n",
    " \n",
    "try:\n",
    "    #  Conectar a Azure SQL\n",
    "    print(f\"Conectando a Azure SQL...\")\n",
    "    conn_azure = pyodbc.connect(azure_conn_str)\n",
    "   \n",
    "    # 🔹 Ejecutar la consulta en Azure SQL\n",
    "    print(f\"Ejecutando consulta en Azure SQL...\")\n",
    "    df = pd.read_sql(SQL_QUERY, conn_azure)\n",
    " \n",
    "    if df.empty:\n",
    "        print(f\" La consulta no devolvió resultados. No se creará la tabla en SQL Server Local.\")\n",
    "    else:\n",
    "        print(f\"   - Datos extraídos: {df.shape[0]} filas\")\n",
    " \n",
    "        #  Convertir NaN en columnas numéricas a 0\n",
    "        df = df.fillna(0)\n",
    " \n",
    "        #  Convertir valores numéricos problemáticos\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype(np.float32)  # Reducir precisión\n",
    "       \n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype(np.int32)  # Evitar valores fuera de rango\n",
    "       \n",
    "        #  Conectar a SQL Server Local\n",
    "        print(f\"Conectando a SQL Server Local...\")\n",
    "        conn_local = pyodbc.connect(local_conn_str)\n",
    "       \n",
    "        with conn_local.cursor() as cursor:\n",
    "            # 🔹 Eliminar la tabla si ya existe\n",
    "            drop_table_sql = f\"DROP TABLE IF EXISTS {NEW_TABLE_NAME}\"\n",
    "            cursor.execute(drop_table_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\"   - Tabla eliminada si existía.\")\n",
    " \n",
    "            # 🔹 Crear la tabla en SQL Server Local con tipos de datos ajustados\n",
    "            columns_sql = []\n",
    "            for col in df.columns:\n",
    "                col_type = df[col].dtype\n",
    "                if np.issubdtype(col_type, np.float32) or np.issubdtype(col_type, np.float64):\n",
    "                    columns_sql.append(f'[{col}] FLOAT')\n",
    "                elif np.issubdtype(col_type, np.int32) or np.issubdtype(col_type, np.int64):\n",
    "                    columns_sql.append(f'[{col}] INT')\n",
    "                else:\n",
    "                    columns_sql.append(f'[{col}] NVARCHAR(255)')\n",
    " \n",
    "            create_table_sql = f\"CREATE TABLE {NEW_TABLE_NAME} ({', '.join(columns_sql)});\"\n",
    "            cursor.execute(create_table_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\"   - Tabla {NEW_TABLE_NAME} creada correctamente en SQL Server Local.\")\n",
    " \n",
    "            # Insertar los datos en SQL Server Local\n",
    "            placeholders = ', '.join(['?' for _ in df.columns])\n",
    "            insert_sql = f\"INSERT INTO {NEW_TABLE_NAME} VALUES ({placeholders})\"\n",
    " \n",
    "            cursor.fast_executemany = True\n",
    "            cursor.executemany(insert_sql, df.values.tolist())\n",
    "            conn_local.commit()\n",
    " \n",
    "            print(f\"   - {df.shape[0]} filas insertadas en {NEW_TABLE_NAME}.\")\n",
    " \n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    " \n",
    "finally:\n",
    "    if 'conn_azure' in locals():\n",
    "        conn_azure.close()\n",
    "    if 'conn_local' in locals():\n",
    "        conn_local.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
